import os
import sys
import torch
import configparser
import urllib.request

from .tacotron2.model import Tacotron2
from .waveglow.model import WaveGlow
from .text import text_to_sequence

from speechbrain.pretrained import Tacotron2 as SpeechBrain
from speechbrain.pretrained import HIFIGAN

config = configparser.ConfigParser()
config.read(os.path.join(os.path.dirname(__file__), "config.cfg"))


def _download_checkpoint(checkpoint, force_reload):
    """
    This method download the tacotron checkpoint weights from the checkpoint url

    Parameters:
    -----------
    checkpoint: String, url to the checkpoint.
    force_reload: Boolean.

    Return:
    -----------
    ckpt_file: String, Path to downloaded checkpoint file

    """
    model_dir = os.path.join(torch.hub._get_torch_home(), "checkpoints")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))
    if not os.path.exists(ckpt_file) or force_reload:
        sys.stderr.write("Downloading checkpoint from {}\n".format(checkpoint))
        urllib.request.urlretrieve(checkpoint, ckpt_file)
    return ckpt_file


def _checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.

    Parameters:
    -----------
    state_dict: Dictionary

    Return:
    -----------
    ret: Boolean

    """
    ret = False
    for key, _ in state_dict.items():
        if key.find("module.") != -1:
            ret = True
            break
    return ret


def _unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.

    Parameters:
    -----------

    state_dict: Dictionary, model's state dict

    Return:
    -----------

    new_state_dict = Dictionary, modified model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace("module.1.", "")
        new_key = new_key.replace("module.", "")
        new_state_dict[new_key] = value
    return new_state_dict


def load_tacotron(checkpoint, force_reload):
    """
    Downloads tacotron2 checkpoints and loading state dictionery

    Parameters:
    -----------
    checkpoint: String, url to the tacotron2 checkpoint.
    force_reload: Boolean. setting this value to true will ignore the downloaded checkpoints from the cache.
    Return:
    -----------
    tacotron2: Model Tacotron
    """
    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    tacotron2 = Tacotron2(**config)
    tacotron2.load_state_dict(state_dict)

    return tacotron2


def load_waveglow(checkpoint, force_reload):
    """
    Downloads waveglow checkpoints and loading state dictionery

    Parameters:
    -----------
    checkpoint: String, url to the waveglow checkpoint.
    force_reload: Boolean. setting this value to true will ignore the downloaded checkpoints from the cache.
    Return:
    -----------
    waveglow: Model, waveglow
    """

    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    waveglow = WaveGlow(**config)
    waveglow.load_state_dict(state_dict)

    return waveglow


def _pretrained_models_file_path(model_name):
    """
    Creating a space in cache to save the downloaded model checkpoint.
    Parameters:
    -----------
    model_name: String, name of the pretrained model.

    Return:
    -----------
    model_ckpt_file: Str, name of the checkpoint directory.
    """
    model_dir = os.path.join(torch.hub._get_torch_home(), "pretrained_models")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_ckpt_file = os.path.join(model_dir, os.path.basename(model_name))
    return model_ckpt_file


def load_seq2seq_model(model_name):
    print(model_name)

    def tacotron2_v1():
        return load_tacotron(config["SEQ2SEQ_MODEL"]["TACOTRON2_V1"], False)

    def tacotron2_v2():
        return SpeechBrain.from_hparams(
            source=config["SEQ2SEQ_MODEL"]["TACOTRON2_v2"],
            savedir=_pretrained_models_file_path("tacotron2_v1"),
        )

    def default():
        return "Incorrect model name"

    if model_name == "tacotron2_v1":
        return tacotron2_v1()
    elif model_name == "tacotron2_v2":
        return tacotron2_v2()
    else:
        return default()


def load_vocorder_model(model_name):
    def waveglow():
        waveglow = load_waveglow(config["VOCORDER"]["WAVEGLOW"], False)
        return waveglow.remove_weightnorm(waveglow)

    def hifigan():
        return HIFIGAN.from_hparams(
            source="speechbrain/tts-hifigan-libritts-22050Hz",
            savedir=_pretrained_models_file_path("hifigan"),
        )

    if model_name == "waveglow":
        return waveglow()
    elif model_name == "hifigan":
        return hifigan()
    else:
        return "The vocorder model is incorrect"


def _pad_sequences(batch):
    # Right zero-pad all one-hot text sequences to max input length
    input_lengths, ids_sorted_decreasing = torch.sort(
        torch.LongTensor([len(x) for x in batch]), dim=0, descending=True
    )
    max_input_len = input_lengths[0]

    text_padded = torch.LongTensor(len(batch), max_input_len)
    text_padded.zero_()
    for i in range(len(ids_sorted_decreasing)):
        text = batch[ids_sorted_decreasing[i]]
        text_padded[i, : text.size(0)] = text

    return text_padded, input_lengths


def prepare_input_sequence(texts, cpu_run=True):

    d = []
    for i, text in enumerate(texts):
        d.append(torch.IntTensor(text_to_sequence(text, ["english_cleaners"])[:]))

    text_padded, input_lengths = _pad_sequences(d)
    if not cpu_run:
        text_padded = text_padded.cuda().long()
        input_lengths = input_lengths.cuda().long()
    else:
        text_padded = text_padded.long()
        input_lengths = input_lengths.long()

    return text_padded, input_lengths
